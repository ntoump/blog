{
  
    
        "post0": {
            "title": "Titanic Part 3",
            "content": "After spending a lot of time experimenting with different hyperparameter options, I came to the realization that I should perhaps leave aside the Random Forest Regressor for the time being, as I did not want to dedicate anymore time studying related literature with seemingly marginal improvements. . Instead, I decided to try out a new type of algorithm; one that is not widely used for Tabular data, and that has quite a few complications both in its training and its tuning in order to achieve a decent model. . That is, of course, Neural Networks. . Neural Networks . I will again be using the fastai library, which adds functionality on top of PyTorch. . [I have been using fastai in every project displayed thus far in this blog. That is mainly because it provides a reliable and resilient API that handles low-level details, thereby allowing the engineer to focus on high-level abstraction, which can result in less implementing and more experimenting - and in case of learning and researching, the more experimenting the merrier.] . Data . I generally followed the same data preprocessing routine (which you can find here) in this implementation as well, with a few largely unimportant exceptions (such as removing redundant columns and swapping the classification of one or two (depending on the model attempted) column names from Categorical to Continuous). . splits = RandomSplitter(valid_pct=0.2)(range_of(df)) to = TabularPandas(df, procs, cat, cont, dep_var, splits=splits, y_block = CategoryBlock) . NNs being NNs require special treatment regarding the format in which the data is inputted in the model. That&#39;s why it is necessary to create DataLoaders batches, before constructing the fastai-powered Learner. . dls = to.dataloaders(bs=64) . Classification rather than Regression . From now on I will be treating the competition as a classification problem, I thereby formatted the data&#39;s labels as categories _(yblock=CategoryBlock). This is the main difference between the data used thus far and the new TabularPandas instance above. . [Later on prepare to have that approach challenged, as I wanted to play around with distinct but not necessarily contrasting paths.] . Concluding the Data Preprocessing . len(to.train),len(to.valid) . (713, 177) . A bigger training set can lead to a better data representation in the model, but a smaller validation set can exclude important features from being accounted for. . Learner &amp; Training . Wherever it is not specified otherwise, consider that the only parameters changed are the ones explicitly mentioned; in other words, all the default values were left as is unless otherwise indicated. . First model . learn = tabular_learner(dls, metrics=accuracy) . Training . A few initial notes: . The model is training unusually fast. But then again, it is training on Tabular data, and, even more so, only a handful of them. | Its accuracy is already decent. | One would expect that the model is going to start overfitting fairly soon, as I have not adjusted the training learning rate (nor will I do so in this routine). | . As expected, the model started overfitting early on. It hit, however, an accuracy of ~84.2% right before that. Perhaps with a few tweaks the next one can become even better. . Review . Inspecting a few random results, we see that the values of the columns Survived and _Survivedpred are almost identical. The labels and the predictions are, therefore, in this small sample fairly well matched, even after training for way too long for the model not to overfit. . Targeted Predictions . I should also mention that it is possible to get the model&#39;s predictions on a specified row (or series of rows) of a DataFrame object. An example case is presented below. . For clarity&#39;s sake: . The variable row contains all the initial row info that the model inputs. | clas contains the model&#39;s prediction regarding the classification of the entry (in this case, it correctly predicts that the person corresponding to this entry will not survive (0 instead of 1)). | probs has the probabilities of each category (in this case we only have two possible prediction outcomes; see here an example of Multi Label Classification using one hot encoding). | . (Unsuccessful) Next models . I experimented for quite a while with different NN training routines that I had found useful in the past. Slowly but surely, I started tweaking the hyperparameters as well, always maintaing the same NN architecture. . The results were either approximately the same or worse than the ones presented above. Until I encountered the Breakthrough. . Breakthrough . What I have taken the liberty of naming as such almost exclusively consists of two revelations I had; . increasing the batch size (bs) is not always a great idea; decreasing it could, in smaller datasets (ds), prove itself extremely beneficial, | the default fastai architecture for Tabular data Learners, while not bad and pretty-much universally applicable, needs to be adapted depending on the peculiarities of each task. | . Conclusion . In this short Part, I introduced Neural Networks as a possibly viable (and perhaps an even better) alternative to Random Forests. In continuation, I will elaborate on the Breakthrough, try out different versions, and, probably, proceed to making the second submission on Kaggle. .",
            "url": "https://ntoump.github.io/blog/2020/10/15/Titanic_PART_3.html",
            "relUrl": "/2020/10/15/Titanic_PART_3.html",
            "date": " • Oct 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Titanic Part 2",
            "content": "Continuing where the Part 1 left off, in this Part I will explore the most popular algorithm for Tabular data (i.e. Random Forests), rapidly mention a few notes on hyperparameters, and pave the way for the more interesting Parts that are yet to come. . First Submission . Before proceeding, it is fitting to firstly mention the practical results of the baseline model I presented in Part 1. . The results of a single Desicion Tree Regressor are not at all great. In fact, just by following the Kaggle recommended gender separation approach (which works based on the fact that most women survive while most men don&#39;t) one can achieve quite a better score than ~64%. . What if I tried next a series of Desicion Trees; namely a Random Forest? Would that help improve the baseline? . Random Forest Regressor . Having already experimented with the Decision Tree Regressor in the previous Part, it only makes sense to try next the Random Forest Regressor algorithm. . Before moving on, however, I would like to demonstrate what happens when you are either misinformed regarding the inner workings of the algorithms you are applying or not cautious enough with the arguments you pass in it. . The &quot;perfect&quot; model . Image(&quot;perfacc.PNG&quot;) . You can see that I have created the perfect model with only three lines of code and minimal effort. . Or have I? That accuracy seems to good to be true, and it is! I just wanted to see how much my model had learned by heart, it turns out that&#39;s almost every entry in the dataset. . That is largely due to the very high number of max leaf nodes, as well as the purposeful lack of other precautions. The model is almost perfectly fitted to the training dataset, which means that it will not generalize well in data that it has not been trained on. . In fewer words, this is an almost perfectly overfitting (thereby useless in real life) model. . The not-so-perfect model(s) . After trying out many unimportant in this context hyperparameter options, I decided to facilitate the experimentation process by creating a template in which I pass in the every time desired options; I therefore created the function below. . def rf(xs, y, n_estimators=40, max_samples=int(len(xs)/2), max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . There are not many things happening in the function above; it merely passes its arguments to the scikit-learn provided Random Forest algorithm, alongside others that have been spelled out for clarity. . Time to use the rf function. . Model 1 . m = rf(xs, y) accuracy = acc(m.predict(xs),y) accuracy . 0.8863955119214586 . That means that the model has absorbed a lot of the data, and is possibly overfitting by a few percentage points. Let&#39;s see what happens with the validation set. . accuracy = acc(m.predict(valid_xs), valid_y) accuracy . 0.8258426966292135 . The accuracy seems decent to say the least. It has already improved on the baseline model&#39;s score of ~82%, albeit not by a significant margin. . Image(&quot;estimatorspeak.PNG&quot;) . Using numpy&#39;s function stack(), we can take an under-the-hood look at the predictions that each estimator (meaning each seperate Decision Tree Regressor) has made regarding each entry of the validation dataset. . It is almost obvious that, generally speaking, there is uniformity (at least in how I deal with regression inference in this context; if r&gt;=0.5 then r=1, else (if r&lt;0.5 then - redundant) r=0). . Model 2 . With slight tweaking we are already getting better results. . m = rf(xs, y, 50, 450, 0.4) acc(m.predict(xs), y) . 0.884992987377279 . acc(m.predict(valid_xs), valid_y) . 0.8314606741573034 . Below we see the estimations regarding a range of 50 estimators; how would setting this hyperparameter as a specific number affect the accuracy compared to setting it as a different number. . Image(&quot;predsplot.PNG&quot;) . Model 3 . Let&#39;s tweak a bit more, just to see if any initial patterns could arise. . m = rf(xs, y, 18, 400, 0.4) acc(m.predict(xs), y) . 0.8751753155680224 . acc(m.predict(valid_xs), valid_y) . 0.8370786516853933 . Not bad! It seems that with every model a gain of ~0.5% accuracy is noted. . Image(&quot;predsplot0.PNG&quot;) . The graph shows that there is a limit to the effect that adding more estimators can have on the accuracy of the model; from 20 onwards there is not a noticeable improvement. . Out-of-bag Error . While catching up with the theory of Random Forests, I could not help but notice the gravity that the Out-of-bag Error seems to have on evaluating the model&#39;s performance more objectively. . The main idea is described here and here, among other places. . To get the OOB predictions in scikit-learn we use the .oobprediction attribute of the Random Forest Regressor. . acc(m.oob_prediction_, y) . 0.8120617110799438 . The OOB predictions are obviously different from the &quot;normal&quot; predictions of the model. We see that this divergence extends to ~2.6% worth of accuracy. In other words, the worst score that I got after spending some time learning and experimenting is probably the most reliable indicator that instead of learning to generalize well, all this time my model has been learning to fit on the validation set instead. . And, naturally, this could not have happened without my aid. . preds.shape . (18, 178) . The shape of the predictions is enlightening; we see the number of validation data (178) alongside 18, which is the number of estimators that the model has been created with. [You can verify that by checking up the first cell of Model 3, where the estimators argument is the third positional argument in order.] . Review . In this part I went over how a few instances (only the most notable of which I mention here to save space, time, and energy) of, probably, the most popular machine learning algorithm in Tabular data (i.e. Random Forests) improve the overall accuracy and reliability of a model. I briefly compared it to the model of the previous Part, and discussed the OOB Error and its effects on this best-case (for now) model. . Conclusion . Next up, we will explore a few other algorithms, start making regular submissions on Kaggle, and perhaps commence delving into Neural Networks and their function in Tabular data. .",
            "url": "https://ntoump.github.io/blog/machine%20learning/jupyter/tabular%20data/scikit-learn/titanic/random%20forests/2020/10/08/Titanic_PART_2.html",
            "relUrl": "/machine%20learning/jupyter/tabular%20data/scikit-learn/titanic/random%20forests/2020/10/08/Titanic_PART_2.html",
            "date": " • Oct 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Titanic Part 1",
            "content": "In this blogpost, I will present a short review of my attempts to tackle the famous Titanic Dataset machine learning problem. . Its goal is, essentially, to predict on a test dataset of 418 people for each person/entry who lives (portrayed as 1) and who dies (as 0) in the tragic incident. The predictions are made on the basis of the passengers&#39; attributes, such as their gender, their age, and the price of their ticket. . Despite this being my first Kaggle competition, I did achieve a decent final ranking of top 5%. Of course, I have strong reasons to believe that with more time and effort, I would easily slide in the top 1%. . Data Preprocessing . Initially, I started with a standard convention approach: keep all columns that are seemingly different and possibly useful, and classify them as Categorical or Continuous. . The Continuous variables are inputted directly in the model, while the Categorical ones first go through a modification process, which assigns to each level in their domain a unique, albeit inherently meaningless, numerical value. . This helps in differentiating between different values (levels) of Categorical variables, as the models learns to consider a specific arithmetic sequence as the identification factor for each, thereby it can detect relations in the data (e.g. more 0&#39;s than 1&#39;s - regarding a specific column - tend to survive) without knowing what the values mean (in this example, that women tend to survive more than men - in the category of gender ). . Input of the Training Dataset . From here on out, the training dataset will be represented as df, as it has been inputted as a Pandas DataFrame. . The same goes for the testing dataset, where t_df stands instead. . sb.heatmap(df.isnull()) . Seeing that there are many null values in the Cabin column, I decided to drop it from the DataFrame. . Also, I interpolated the null values of the Age column. This is a rather common occurence in Tabular data handling; the default is to either set the mean of the column (which wouldn&#39;t help in this case) or to set a specific numerical value for all N/A values (which is preferred here). . df[&#39;Age&#39;] = df[&#39;Age&#39;].interpolate() df = df.drop(&#39;Cabin&#39;,axis=1) df = df.dropna() . In continuation, I: . define the Tabular Procs (as fastai calls the transforms that take place on Tabular data), | set the dependent column (&quot;Survived&quot; , where, as I mentioned above, 1 means &quot;true&quot; and 0 means &quot;false), | seperate the Continuous from the Categorical columns, | remove the Name and the PassengerId columns, since their values are not (contrary to some weird data indications) related to the predictions, | split the training and validation set (with the ratio being 0.75-0.25), | build the TabularPandas object instance, passing in all the above as its parameters. | . procs=[Categorify,FillMissing,Normalize] d_v=&#39;Survived&#39; cont,cat= cont_cat_split(df,dep_var=d_v) cont.remove(&#39;PassengerId&#39;) cat.remove(&#39;Name&#39;) splits=RandomSplitter(0.25)(range_of(df)) to = TabularPandas(df,procs,cat, cont,d_v,CategoryBlock,splits) . Input of the Test Dataset . Seeing that the test dataset is structurally the same as the training dataset, I must apply the same transforms and data preprocessing in order to avoid rather annoying errors that can (and more often than not, do (!)) occur during inference, due to asymmetrical dataset structures. . t_df = pd.read_csv(path/&#39;test.csv&#39;) t_df = t_df.drop(&#39;Cabin&#39;,axis=1) t_df[&#39;Age&#39;]=t_df[&#39;Age&#39;].interpolate() t_df=t_df.fillna(0.0) too = TabularPandas(t_df,procs, cat, cont, y_block=CategoryBlock) . I should mention that, instead of just dropping whatever rows do not have values, instead I call t df.fillna(0.0) to fill every N/A value with 0.0. This must be done, because since the entries are in a specific order, their corresponding predictions must also follow that same order. . First attempt - Decision Tree Regressor . In this first attempt, I just wanted to get my feet wet regarding the dataset and where my expectations should lie. . Below, you can see that the code to train such an algorithm is easily understood and reproduced: . firstly, I assign shorter variable names to signal the splitting of the training set (in two parts: the data of off which the models makes a prediction, and the labels, the value that it tries to predict), | I repeat the process for the validation set, | I create a very simple model with a single customized argument, | and I fit it on the training set. | . Training . xs, y = to.train.xs, to.train.y valid_xs, valid_y = to.valid.xs, to.valid.y m = DecisionTreeRegressor(max_leaf_nodes=30) m.fit(xs, y); . [Unfortunately, I don&#39;t have the luxury of showing the sketches of the tree (and later on, trees (!)) in action, as the graphs are way too big to fit in a single screen. However, I do provide below the code to recreate them. Keep in mind that you may have to adjust the parameters for the drawing to fit in your display.] . draw_tree(m, xs, size=10, leaves_parallel=True, precision=2) . samp_idx = np.random.permutation(len(y))[:500] dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . Testing . As I am performing regression, I must interpret the results of the model in a way that makes sense for the labels of this dataset (which are exclusively 0 and 1). Therefore, I created this very simple function that inputs some array (or even Tensor (!)) of predictions, distinguishes between correct and wrong estimations, and returns the accuracy of the model on the validation data. . def acc(xs): correct, total = 0, 0 for pr,lab in zip(xs, y): if float(pr)&gt;=0.5: pr = 1.0 else: pr = 0.0 if int(pr) == lab: correct += 1 total += 1 return float(correct/total) . accuracy = acc(m.predict(valid_xs), valid_y) accuracy . output: 0.8202247191011236 . Model Overview . The model has a very decent accuracy on the validation data, considering the effort that led to its creation. As it is, however, but a baseline model, I will try to improve on that initial score of ~82.02% before submitting on Kaggle. . Conclusion . In this first part, I briefly went over the dataset, its structure and contents, as well as the initial data processing ritual that I followed in my first submissions. We also roughly saw my first attempt at a model; utilizing non other than the Decision Tree Regressor (which is generously provided by scikit-learn). . In the next parts, I&#39;ll go over how I implemented and fine-tuned more complex algorithms (as Random Forests, (Deep) Neural Networks, and Gradient Boosting Machines, as well as Ensembles of multiple kinds of algorithms) in pursuit of higher accuracy and, overall, better Kaggle ranking. .",
            "url": "https://ntoump.github.io/blog/machine%20learning/jupyter/tabular%20data/scikit-learn/titanic/2020/10/01/Titanic_PART_1.html",
            "relUrl": "/machine%20learning/jupyter/tabular%20data/scikit-learn/titanic/2020/10/01/Titanic_PART_1.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Mask Classifier - Version 3 - PART 1",
            "content": "You can find the free to use, deployed application at https://protected-mountain-28715.herokuapp.com. . Contact Information . Email: toump.nick@gmail.com GitHub: https://github.com/ntoump/ Youtube: https://www.youtube.com/channel/UCFgF1lHh0fRQxY9CqpyBZgw . Training Walkthrough . I should mention that, in this Walkthrough, as well as in the deployed application (https://protected-mountain-28715.herokuapp.com), I use Multi-Label Classification alongside one-hot encoding, so that each image is not limited to one category as output. Instead, the theoritical maximum output is all categories on which the model has been trained (although that is extremely unlikely), and the theoritical minimum is none of these categories (as the model may not be confident enough in any of its predictions). In this last case, as it&#39;s highlighted in the Specification of the application, the model will output an error message. . Data . df = pd.read_csv(&#39;CSVs/only_images4.csv&#39;) df . fname labels . 0 C001.jpg | Cloth | . 1 C002.jpg | Cloth | . 2 C003.jpg | Cloth | . 3 C004.jpg | Cloth | . 4 C005.jpg | Cloth | . ... ... | ... | . 1011 No094.jpg | No_mask! | . 1012 No096.jpg | No_mask! | . 1013 No097.jpg | No_mask! | . 1014 No099.jpg | No_mask! | . 1015 No100.jpg | No_mask! | . 1016 rows × 2 columns . First try . This is my first try of the &quot;masksv3&quot; (i.e. the 3rd Version of this application) at creating an ever better model by using Deep Convolutional Neural Networks, continuing exactly where &quot;masksv2&quot; left off. . In comparison to the previous Version, in this first try I only changed the dataset, filtering out many mislabeled images and adding a few, as well as customizing my previous dataset to support one-hot encoding. At the same time, everything else (e.g. the hyperparameters) is left as is. . After reviewing the final accuracy of this first model, I&#39;ll try to improve on that by tweaking the hyperparameters. . Wish me luck! . # Get dblock, dsets, and dls # Notice that no Presizing is used (all credits to fastai for their incredible work) in this first DataBlock dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=RandomSplitter(), get_x=get_x, get_y=get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) dsets = dblock.datasets(df) dls = dblock.dataloaders(df) . NameError Traceback (most recent call last) &lt;ipython-input-1-b0a99ccd95eb&gt; in &lt;module&gt; 1 # Get dblock, dsets, and dls 2 # Notice that no Presizing is used (all credits to fastai) -&gt; 3 dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), 4 splitter=RandomSplitter(), 5 get_x=get_x, NameError: name &#39;DataBlock&#39; is not defined . # Everything looks good dls.show_batch() . Training . I start off with a small threshold (0.2), ResNet50, and a semi-random base learning rate. Furthermore, I train using _finetune, because it could be a good starting point to begin to understand what the differences are (in practical terms and applications) between the fastai model training tools (including fit, _fit_onecycle, and _finetune). . First set of epochs . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.920142 | 0.598956 | 0.413793 | 00:03 | . 1 | 0.776364 | 0.499953 | 0.561576 | 00:03 | . 2 | 0.672306 | 0.435738 | 0.629310 | 00:03 | . 3 | 0.583970 | 0.318443 | 0.676724 | 00:03 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.314077 | 0.264383 | 0.758005 | 00:03 | . 1 | 0.261582 | 0.194348 | 0.862685 | 00:03 | . 2 | 0.220241 | 0.161082 | 0.887315 | 00:03 | . The accuracy seems to have improved greatly in a few seconds (because of using a P5000 NVIDIA GPU) and epochs of training, which indicates that there was plenty of room to grow . Next, I sketch the lr to loss relationship, and get the suggested lrs, in order to train even further with a better informed lr guess. . learn.lr_find() . SuggestedLRs(lr_min=0.0003019951749593019, lr_steep=0.0006918309954926372) . There does not seem to be a clear slope downwards, that&#39;s why I&#39;ll just use the lr_min. . Second set of epochs . learn.fine_tune(3, base_lr=0.0003019951749593019, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.169711 | 0.157992 | 0.887931 | 00:03 | . 1 | 0.168780 | 0.148448 | 0.906404 | 00:03 | . 2 | 0.160508 | 0.140320 | 0.914409 | 00:03 | . 3 | 0.153258 | 0.132069 | 0.927340 | 00:03 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.133067 | 0.133076 | 0.929803 | 00:03 | . 1 | 0.130006 | 0.130573 | 0.934729 | 00:03 | . 2 | 0.129661 | 0.129405 | 0.937192 | 00:03 | . The accuracy has skyrocketed to almost 94%. Thus far, I can say that we&#39;ve done pretty well. . lr_min, lr_steep = learn.lr_find() . Let&#39;s train some more. . Third and final set of epochs . learn.fine_tune(2, base_lr=lr_min) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.125494 | 0.127268 | 0.939655 | 00:03 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.112003 | 0.122138 | 0.944581 | 00:03 | . 1 | 0.111319 | 0.120099 | 0.943965 | 00:03 | . lr_min, lr_steep = learn.lr_find() . Not bad! We just saw, for the first time, a decrease in accuracy (something that I was expecting, that&#39;s why I trained it only a few epochs per set). That&#39;s not necessarily a sign of overfitting. It would be a good opportunity, though, to get the predictions of the model, and plot the threshold to accuracy relationship. . preds,targs = learn.get_preds() . xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . The above plot shows that there is a positive relationship between threshold and accuracy. Therefore, the higher is the chosen threshold, the higher is expected to be the accuracy of the model. It is also important to note that there isn&#39;t such a great difference between choosing, say, 0.3 and 0.85 as your threshold, perhaps not even a percentage point. . First try - Review . I am fairly impressed by the model&#39;s performance. Reaching 95% accuracy, considering the variation of the ds *, the limited training time (&amp; costs), and the uncustomized hyperparameter choices, is almost excellent. . Now onwards to manual testing. . I have a few dozen images, against which I will check my model&#39;s accuracy. These are the ones used for evaluating the masksv2 application&#39;s model as well. . *: The dataset contains, at this point, seven (#7+1) distinct categories, namely: (#7) [&#39;-&#39;,&#39;Cloth&#39;,&#39;Gas&#39;,&#39;KN95&#39;,&#39;Medical&#39;,&#39;No_mask!&#39;,&#39;Shield&#39;, &#39;Surgical&#39;]. . Each category refers to a type of face mask, except for &quot;-&quot; and &quot;No_mask!&quot;. The last one suggests that there is a maskless face in the given image, whereas the first one (&quot;-&quot;) is part of an experiment I conduct named &quot;Intended Noise&quot; (see below). . Manual Testing . I have three folders of images for my manual testing: &quot;NEW&quot;, &quot;NEU&quot;, and all the other pictures you see below. . [The names of the images are supposed to be related to their content, while that is not always necessarily the case.] . ls . NEU/ c.jpg c1.jpg c3.jpg f.jpg ff0.jpg kn.jfif kn1.jfif kn3.jfif NEW/ c0.jpg c2.jpg c4.jpg ff.jfif k.jfif kn0.jfif kn2.jpg m.jpg . # Built a quick loop to do the work for me # Below you can see the predictions of the model import os pather = Path(&#39;/notebooks/Manual&#39;) for img in os.listdir(pather): if not img[0] == &#39;.&#39;: if not img[0].isupper(): print(img, learn.predict(img)) . c.jpg ((#1) [&#39;Surgical&#39;], tensor([False, False, False, False, False, False, False, True]), tensor([0.0775, 0.1155, 0.0063, 0.0130, 0.0673, 0.0341, 0.2313, 0.9998])) . c0.jpg ((#2) [&#39;Cloth&#39;,&#39;Surgical&#39;], tensor([False, True, False, False, False, False, False, True]), tensor([0.0728, 0.9934, 0.0024, 0.0257, 0.2283, 0.0275, 0.0254, 0.7986])) . c1.jpg ((#1) [&#39;Cloth&#39;], tensor([False, True, False, False, False, False, False, False]), tensor([0.1199, 0.9999, 0.0045, 0.0559, 0.1601, 0.0538, 0.0173, 0.0220])) . c2.jpg ((#2) [&#39;Cloth&#39;,&#39;Surgical&#39;], tensor([False, True, False, False, False, False, False, True]), tensor([0.0886, 0.9984, 0.0017, 0.0033, 0.1045, 0.0977, 0.2173, 0.6740])) . c3.jpg ((#1) [&#39;Cloth&#39;], tensor([False, True, False, False, False, False, False, False]), tensor([0.0635, 1.0000, 0.0062, 0.0124, 0.1405, 0.0456, 0.0129, 0.0107])) . c4.jpg ((#1) [&#39;KN95&#39;], tensor([False, False, False, True, False, False, False, False]), tensor([0.0337, 0.4820, 0.0034, 0.9528, 0.1218, 0.0256, 0.0292, 0.2378])) . f.jpg ((#1) [&#39;Shield&#39;], tensor([False, False, False, False, False, False, True, False]), tensor([9.1857e-02, 1.9036e-02, 7.7392e-04, 4.6145e-03, 9.7188e-02, 8.0676e-03, 1.0000e+00, 1.9384e-01])) . ff.jfif ((#2) [&#39;Gas&#39;,&#39;KN95&#39;], tensor([False, False, True, True, False, False, False, False]), tensor([0.0387, 0.0341, 0.9998, 0.7817, 0.1651, 0.0845, 0.0285, 0.0202])) . ff0.jpg ((#2) [&#39;Shield&#39;,&#39;Surgical&#39;], tensor([False, False, False, False, False, False, True, True]), tensor([0.0545, 0.0850, 0.0047, 0.1072, 0.2291, 0.0347, 0.9999, 1.0000])) . k.jfif ((#2) [&#39;Cloth&#39;,&#39;Shield&#39;], tensor([False, True, False, False, False, False, True, False]), tensor([0.0819, 0.9689, 0.0013, 0.1041, 0.1234, 0.0768, 0.6258, 0.0066])) . kn.jfif ((#1) [&#39;KN95&#39;], tensor([False, False, False, True, False, False, False, False]), tensor([0.0695, 0.0832, 0.0131, 0.5997, 0.2483, 0.0758, 0.0988, 0.1063])) . kn0.jfif ((#1) [&#39;KN95&#39;], tensor([False, False, False, True, False, False, False, False]), tensor([0.0559, 0.1427, 0.0032, 0.9986, 0.1310, 0.0318, 0.0668, 0.0511])) . kn1.jfif ((#1) [&#39;KN95&#39;], tensor([False, False, False, True, False, False, False, False]), tensor([0.1210, 0.0280, 0.0057, 1.0000, 0.1297, 0.0482, 0.0182, 0.0055])) . kn2.jpg ((#2) [&#39;KN95&#39;,&#39;Surgical&#39;], tensor([False, False, False, True, False, False, False, True]), tensor([0.0345, 0.0628, 0.0126, 0.5065, 0.1243, 0.0679, 0.0438, 0.9757])) . kn3.jfif ((#1) [&#39;KN95&#39;], tensor([False, False, False, True, False, False, False, False]), tensor([0.0193, 0.0180, 0.0032, 1.0000, 0.1066, 0.0293, 0.0219, 0.0073])) . m.jpg ((#1) [&#39;Surgical&#39;], tensor([False, False, False, False, False, False, False, True]), tensor([0.0604, 0.0252, 0.0092, 0.0068, 0.1327, 0.1073, 0.0532, 1.0000])) . for img in os.listdir(pather/&#39;NEW&#39;): print(img, learn.predict(img)) . 0.jpg ((#1) [&#39;Cloth&#39;], tensor([False, True, False, False, False, False, False, False]), tensor([0.2064, 0.7891, 0.0167, 0.0831, 0.1360, 0.0263, 0.1972, 0.0074])) . 1.jpg ((#1) [&#39;Medical&#39;], tensor([False, False, False, False, True, False, False, False]), tensor([0.0397, 0.0667, 0.0052, 0.0510, 0.8644, 0.0356, 0.0485, 0.0339])) . 10.jpg ((#2) [&#39;Cloth&#39;,&#39;Surgical&#39;], tensor([False, True, False, False, False, False, False, True]), tensor([0.0320, 0.9658, 0.0103, 0.1079, 0.3641, 0.0235, 0.0414, 0.6926])) . 100.jpg ((#2) [&#39;Cloth&#39;,&#39;Medical&#39;], tensor([False, True, False, False, True, False, False, False]), tensor([0.0284, 0.9688, 0.0715, 0.4539, 0.9123, 0.2135, 0.0244, 0.0144])) . 3.jpg ((#1) [&#39;Medical&#39;], tensor([False, False, False, False, True, False, False, False]), tensor([0.0089, 0.1283, 0.0151, 0.3042, 0.9984, 0.1028, 0.0139, 0.0350])) . 30.jpg ((#3) [&#39;Cloth&#39;,&#39;KN95&#39;,&#39;Medical&#39;], tensor([False, True, False, True, True, False, False, False]), tensor([0.0420, 0.9991, 0.0048, 0.6708, 0.9817, 0.0964, 0.0404, 0.0023])) . for img in os.listdir(pather/&#39;NEU&#39;): print(img, learn.predict(img)) . f.jpg ((#1) [&#39;No_mask!&#39;], tensor([False, False, False, False, False, True, False, False]), tensor([0.0688, 0.0212, 0.0060, 0.0261, 0.1245, 1.0000, 0.0367, 0.0180])) . f0.jpg ((#1) [&#39;No_mask!&#39;], tensor([False, False, False, False, False, True, False, False]), tensor([0.0323, 0.0214, 0.0043, 0.0135, 0.0879, 1.0000, 0.0364, 0.0129])) . f1.jpg ((#1) [&#39;No_mask!&#39;], tensor([False, False, False, False, False, True, False, False]), tensor([0.0420, 0.0315, 0.0054, 0.0091, 0.0844, 1.0000, 0.0286, 0.0094])) . f2.jpg ((#1) [&#39;No_mask!&#39;], tensor([False, False, False, False, False, True, False, False]), tensor([0.0301, 0.0200, 0.0069, 0.0065, 0.1041, 1.0000, 0.0287, 0.0204])) . f3.jpg ((#1) [&#39;No_mask!&#39;], tensor([False, False, False, False, False, True, False, False]), tensor([0.0465, 0.0427, 0.0093, 0.0152, 0.1351, 1.0000, 0.0774, 0.0113])) . f4.jpg ((#1) [&#39;No_mask!&#39;], tensor([False, False, False, False, False, True, False, False]), tensor([0.0402, 0.0260, 0.0057, 0.0118, 0.0923, 1.0000, 0.0645, 0.0311])) . f5.jpg ((#1) [&#39;No_mask!&#39;], tensor([False, False, False, False, False, True, False, False]), tensor([0.0531, 0.0342, 0.0030, 0.0058, 0.1081, 1.0000, 0.0260, 0.0279])) . i0.jpg ((#1) [&#39;-&#39;], tensor([ True, False, False, False, False, False, False, False]), tensor([0.9999, 0.0137, 0.0048, 0.0347, 0.0524, 0.0716, 0.0443, 0.0584])) . i1.jpg ((#1) [&#39;-&#39;], tensor([ True, False, False, False, False, False, False, False]), tensor([0.9998, 0.2033, 0.0035, 0.0099, 0.0363, 0.0421, 0.0947, 0.0159])) . i2.jpg ((#2) [&#39;-&#39;,&#39;Shield&#39;], tensor([ True, False, False, False, False, False, True, False]), tensor([0.9941, 0.1205, 0.0118, 0.0185, 0.1548, 0.0936, 0.5295, 0.0373])) . i3.jpg ((#1) [&#39;-&#39;], tensor([ True, False, False, False, False, False, False, False]), tensor([1.0000, 0.0770, 0.0095, 0.0109, 0.1565, 0.0578, 0.0114, 0.0063])) . i4.jpg ((#1) [&#39;-&#39;], tensor([ True, False, False, False, False, False, False, False]), tensor([1.0000, 0.0118, 0.0027, 0.0067, 0.1234, 0.1374, 0.0240, 0.0239])) . l0.jpg ((#1) [&#39;-&#39;], tensor([ True, False, False, False, False, False, False, False]), tensor([1.0000, 0.0766, 0.0029, 0.0092, 0.0801, 0.0751, 0.0630, 0.0171])) . l1.jpg ((#2) [&#39;-&#39;,&#39;Cloth&#39;], tensor([ True, True, False, False, False, False, False, False]), tensor([1.0000, 0.8937, 0.0038, 0.1679, 0.3973, 0.0362, 0.0091, 0.0034])) . l2.jpg ((#1) [&#39;-&#39;], tensor([ True, False, False, False, False, False, False, False]), tensor([1.0000, 0.1106, 0.0068, 0.0163, 0.1903, 0.0247, 0.0390, 0.0029])) . l3.jpg ((#1) [&#39;-&#39;], tensor([ True, False, False, False, False, False, False, False]), tensor([1.0000, 0.0251, 0.0066, 0.0144, 0.1485, 0.0422, 0.0181, 0.0456])) . l4.jpg ((#1) [&#39;-&#39;], tensor([ True, False, False, False, False, False, False, False]), tensor([1.0000, 0.2441, 0.0053, 0.0287, 0.0584, 0.0346, 0.0988, 0.0175])) . Manual Check - Review . Spare images . The model did not do particularly well with the spare images. I&#39;ll get back to that very soon. . &quot;NEW&quot; . The &quot;NEW&quot; folder contains images with many people wearing masks, rather than just one person at a time (the dataset contained only such images at this point; with one (mostly) mask per image). It was at this point, while reviewing these results, that I realized my blunder; I had included two categories in the dataset for the same type of masks (namely, &quot;Surgical&quot; and &quot;Medical&quot;). . This was obviously done by accident, as I had not found a comfortable API to clean the data (the fastai one does not work for multi-label classification), therefore I had to do it by hand. That gave me, nontheless, an idea; what if I included all &quot;Surgical&quot; images with the &quot;Medical&quot; labeled ones into a common category, named &quot;Medical&quot; (To be continued...)? . &quot;NEU&quot; . Now that&#39;s where the model really shined. You see, the images in this category are part of what I call &quot;Intended Noise&quot; (DISCLAIMER: I have not encountered this idea anywhere in DL, that&#39;s why I have taken the liberty of naming it, and I don&#39;t provide citation. If anyone knows if that&#39;s a thing, please reach out to me with the relevant sources!). . I&#39;m not going to delve into that in this post. Keep an eye out for that, though, because a detailed approach to explaining this idea, as well as its potential benefits to DL models, will be posted soon. . Interpretation . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(41, nrows=10) . target predicted probabilities loss . 0 Shield;Surgical | Cloth;Medical | tensor([0.0077, 0.9984, 0.0078, 0.2098, 0.9549, 0.0643, 0.0777, 0.0062]) | 2.187938690185547 | . 1 Cloth | Cloth;Medical | tensor([0.0247, 0.9129, 0.0176, 0.3189, 0.9997, 0.0585, 0.0198, 0.0138]) | 1.072890043258667 | . 2 KN95 | Cloth | tensor([0.0213, 0.9823, 0.0130, 0.0497, 0.1453, 0.1231, 0.2686, 0.1380]) | 0.9774149656295776 | . 3 Cloth | Cloth;KN95;Shield | tensor([0.0695, 0.5541, 0.0023, 0.8898, 0.0520, 0.0520, 0.9902, 0.0553]) | 0.9573626518249512 | . 4 KN95 | Cloth | tensor([0.1523, 0.9844, 0.0093, 0.1110, 0.0942, 0.0725, 0.0575, 0.0278]) | 0.8492633104324341 | . 5 Cloth;Medical | - | tensor([0.9499, 0.3249, 0.0056, 0.0223, 0.3289, 0.0488, 0.0469, 0.0617]) | 0.6774997711181641 | . 6 Medical | KN95;Surgical | tensor([0.0414, 0.0395, 0.0103, 0.8503, 0.1052, 0.0268, 0.0749, 0.6215]) | 0.6650393009185791 | . 7 Cloth;KN95;Medical | Cloth;Medical | tensor([0.0505, 0.9252, 0.0441, 0.0282, 0.7488, 0.0972, 0.0126, 0.0173]) | 0.5204622149467468 | . 8 Shield;Surgical | Surgical | tensor([0.0435, 0.0351, 0.0060, 0.0195, 0.1074, 0.0586, 0.0225, 1.0000]) | 0.5091105103492737 | . 9 Gas | Gas;Shield | tensor([0.0169, 0.1474, 0.9952, 0.0050, 0.1380, 0.1350, 0.9696, 0.0539]) | 0.5035613179206848 | . 10 Medical | Cloth | tensor([0.0402, 0.9010, 0.0150, 0.1106, 0.2715, 0.0434, 0.0132, 0.0726]) | 0.49044445157051086 | . 11 Cloth | KN95;Medical | tensor([0.0313, 0.3274, 0.0054, 0.8075, 0.5183, 0.0214, 0.0184, 0.0662]) | 0.4551154375076294 | . 12 KN95 | Cloth | tensor([0.0629, 0.7515, 0.0389, 0.1632, 0.1261, 0.0354, 0.0563, 0.0210]) | 0.44490036368370056 | . 13 Medical | | tensor([0.1121, 0.3474, 0.0093, 0.2354, 0.0976, 0.0687, 0.0074, 0.2106]) | 0.4331527352333069 | . 14 Cloth;Medical;Shield | Cloth;Medical | tensor([0.0250, 0.9994, 0.0053, 0.0496, 0.8723, 0.0696, 0.0463, 0.0132]) | 0.42210978269577026 | . 15 Medical | Cloth | tensor([0.0294, 0.7738, 0.0484, 0.0403, 0.2920, 0.1259, 0.1379, 0.0157]) | 0.3921005129814148 | . 16 Cloth;KN95;Medical | Cloth;Medical | tensor([0.0176, 0.5974, 0.0060, 0.0978, 1.0000, 0.0401, 0.0211, 0.0606]) | 0.3735991418361664 | . 17 KN95 | KN95;Shield | tensor([0.0200, 0.1745, 0.0029, 0.9935, 0.1081, 0.0765, 0.8843, 0.2139]) | 0.35156717896461487 | . 18 Shield | KN95;Shield | tensor([0.0181, 0.0419, 0.0024, 0.9099, 0.0988, 0.0316, 0.9965, 0.0137]) | 0.327982634305954 | . 19 Surgical | - | tensor([0.7569, 0.0316, 0.0045, 0.0167, 0.1484, 0.0656, 0.0061, 0.4032]) | 0.32634592056274414 | . 20 Surgical | | tensor([0.0282, 0.4589, 0.0152, 0.0391, 0.1623, 0.1415, 0.1643, 0.2511]) | 0.32359299063682556 | . 21 No_mask! | Cloth;No_mask! | tensor([0.3992, 0.7288, 0.0027, 0.0110, 0.1893, 0.8107, 0.0027, 0.1024]) | 0.2948143184185028 | . 22 Cloth | Cloth;Surgical | tensor([0.0578, 0.6261, 0.0037, 0.0493, 0.1708, 0.0303, 0.1064, 0.6651]) | 0.25082236528396606 | . 23 Cloth | | tensor([0.0534, 0.4188, 0.0059, 0.4405, 0.1227, 0.1601, 0.0911, 0.0557]) | 0.24625501036643982 | . 24 Cloth | | tensor([0.2052, 0.2710, 0.0556, 0.0713, 0.1052, 0.0408, 0.0304, 0.0265]) | 0.23463144898414612 | . 25 No_mask! | -;No_mask! | tensor([0.5985, 0.3772, 0.0043, 0.0417, 0.2019, 0.9302, 0.0539, 0.0099]) | 0.22453458607196808 | . 26 Cloth | Cloth;Surgical | tensor([0.0708, 0.9986, 0.0024, 0.0011, 0.2059, 0.0593, 0.0077, 0.7262]) | 0.2091251015663147 | . 27 Surgical | Shield;Surgical | tensor([0.0698, 0.0179, 0.0041, 0.0595, 0.1548, 0.0284, 0.7116, 1.0000]) | 0.19954973459243774 | . 28 Cloth | Cloth;Medical | tensor([0.1025, 0.9139, 0.0071, 0.0101, 0.6403, 0.0783, 0.0440, 0.0036]) | 0.17099031805992126 | . 29 KN95 | KN95 | tensor([0.2454, 0.1503, 0.0077, 0.8222, 0.2953, 0.0441, 0.1083, 0.0736]) | 0.15426596999168396 | . 30 Shield;Surgical | Shield | tensor([0.0613, 0.0185, 0.0017, 0.0955, 0.1229, 0.0527, 1.0000, 0.4334]) | 0.15068519115447998 | . 31 Shield;Surgical | Shield | tensor([0.0613, 0.0185, 0.0017, 0.0955, 0.1229, 0.0527, 1.0000, 0.4334]) | 0.15068519115447998 | . 32 Cloth | Cloth | tensor([0.0271, 0.9734, 0.0240, 0.1700, 0.4952, 0.0207, 0.0316, 0.1311]) | 0.14279848337173462 | . 33 - | - | tensor([0.9376, 0.0965, 0.4565, 0.0216, 0.0858, 0.0359, 0.0380, 0.0883]) | 0.13187271356582642 | . 34 Surgical | Surgical | tensor([0.0828, 0.3826, 0.0157, 0.0843, 0.1102, 0.0422, 0.1392, 0.9757]) | 0.1258525252342224 | . 35 Cloth | Cloth;Medical | tensor([0.0189, 1.0000, 0.0043, 0.0043, 0.5717, 0.0649, 0.0155, 0.0093]) | 0.12096662819385529 | . 36 Cloth | Cloth | tensor([0.1844, 0.9992, 0.0032, 0.0060, 0.2830, 0.0134, 0.2912, 0.0589]) | 0.12058921158313751 | . 37 - | - | tensor([0.9853, 0.3103, 0.0036, 0.0394, 0.1954, 0.1406, 0.0307, 0.0200]) | 0.10630811750888824 | . 38 Cloth | Cloth | tensor([0.1158, 0.9534, 0.0564, 0.1340, 0.1559, 0.0955, 0.1095, 0.0608]) | 0.10266871750354767 | . 39 - | -;Medical | tensor([0.9997, 0.0311, 0.0061, 0.0070, 0.5063, 0.0262, 0.0277, 0.0118]) | 0.10216984152793884 | . 40 Surgical | Surgical | tensor([0.4630, 0.0049, 0.0096, 0.0046, 0.0630, 0.0538, 0.0532, 1.0000]) | 0.10198771208524704 | . After interpreting the results (for some wicked reason I can&#39;t seem to get the &quot;Confusion matrix&quot; to work with my ds) by viewing the top losses, I realize that the model misses a few easy ones, all the while being extremely accurate at useless datapoints (more on that later). . It should be noted that I DID NOT train the first model until overfitting. I stopped mainly because I had ideas I deemed more valuable to explore, and because of the threshold to accuracy relationship, which suggested that the model is almost as good as possible. Nowadays, I tend to dismiss such early stoppings, given that these ploted relationships are but predictions. | . Second Try . Firstly, I cleaned the ds, as I thought I had done earlier. I merged the &quot;Surgical&quot; and &quot;Medical&quot; categories into &quot;Medical&quot;, and relabeled a few misclassified items. . In continuation, I will tweak slightly the hyperparameters to try to achieve better performance than the above reached (~95%), including cropping to a bigger size (224 to start with) and using Presizing, using a deeper arch, and adjusting the threshold/lr. . Note: I deleated the 224 crop results accidentally during training. It was slightly better than the above reached accuracy, even though only by a percentage point (~96%). I have included, though, below the next stage; applying Presizing with far higher resolution in the hopes of achieving near perfect accuracy. . Data . df = pd.read_csv(&#39;CSVs/only_images4.csv&#39;) df . fname labels . 0 C001.jpg | Cloth | . 1 C002.jpg | Cloth | . 2 C003.jpg | Cloth | . 3 C004.jpg | Cloth | . 4 C005.jpg | Cloth | . ... ... | ... | . 1009 No094.jpg | No_mask! | . 1010 No096.jpg | No_mask! | . 1011 No097.jpg | No_mask! | . 1012 No099.jpg | No_mask! | . 1013 No100.jpg | No_mask! | . 1014 rows × 2 columns . # Here is the main difference of this time; use of Presizing (720-&gt;460), in the hopes of higher accuracy due to higher resolution dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=RandomSplitter(seed=42), get_x=get_x, get_y=get_y, item_tfms=Resize(720), batch_tfms=aug_transforms(size=460, min_scale=0.75)) dsets = dblock.datasets(df) # I got a CUDA memory error; that&#39;s why the batch size had to be smaller than 64, and I traditionally chose 32 dls = dblock.dataloaders(df, bs=32) . dls.show_batch() . Training . # Threshold = 0.8 (instead of 0.2) learn0 = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.8)) . I tried to first get the lr_min, in order to better direct the training. Pretty much expected curve . lr_min, lr_steep = learn0.lr_find() . Cuda memory error . t = torch.cuda.get_device_properties(0).total_memory c = torch.cuda.memory_cached(0) a = torch.cuda.memory_allocated(0) # f = c-a # free inside cache t, c, a . /opt/conda/envs/fastai/lib/python3.8/site-packages/torch/cuda/memory.py:344: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved warnings.warn( . (17069309952, 14545846272, 112194560) . First set of epochs . learn0.fine_tune(3, base_lr=lr_min, freeze_epochs=3) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.707048 | 0.434021 | 0.922207 | 00:27 | . 1 | 0.450835 | 0.140279 | 0.944130 | 00:27 | . 2 | 0.285841 | 0.141349 | 0.962518 | 00:28 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.218881 | 0.932779 | 0.888968 | 00:37 | . 1 | 0.220071 | 0.265597 | 0.937058 | 00:37 | . 2 | 0.172344 | 0.096667 | 0.958274 | 00:37 | . lr_min, lr_st = learn0.lr_find() . Second set of epochs . learn0.fine_tune(2, base_lr=lr_min, freeze_epochs=3) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.101329 | 0.086775 | 0.960396 | 00:28 | . 1 | 0.095762 | 0.086316 | 0.959689 | 00:28 | . 2 | 0.089909 | 0.086750 | 0.959689 | 00:28 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.095427 | 0.088182 | 0.959689 | 00:37 | . 1 | 0.091889 | 0.085616 | 0.961103 | 00:37 | . preds,targs = learn0.get_preds() xs = torch.linspace(0.01,0.99,350) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . Third and final set of epochs . learn0.fine_tune(2, base_lr=lr_min, freeze_epochs=2) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.097029 | 0.089718 | 0.958982 | 00:28 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.094402 | 0.089689 | 0.960396 | 00:37 | . 1 | 0.093266 | 0.086641 | 0.961811 | 00:37 | . preds,targs = learn0.get_preds() xs = torch.linspace(0.05,0.95,350) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . I increase the number of points to search from up to 350. The difference in plot shapes is obvious, albeit of limited practical importance. . Interpretation . interp = ClassificationInterpretation.from_learner(learn0) interp.plot_confusion_matrix() . interp.plot_top_losses(40, nrows=10) . target predicted probabilities loss . 0 - | Medical | tensor([7.1864e-03, 1.9517e-02, 5.7940e-03, 6.6158e-03, 9.1515e-01, 6.4330e-04, 7.1826e-02]) | 1.0728305578231812 | . 1 Medical;Shield | - | tensor([9.4860e-01, 8.2339e-02, 7.8895e-03, 2.3476e-02, 1.2792e-01, 1.5864e-04, 1.0363e-01]) | 1.0584402084350586 | . 2 KN95;Shield | Cloth | tensor([0.0024, 0.8134, 0.0098, 0.1013, 0.1418, 0.0074, 0.1106]) | 0.9061436653137207 | . 3 Cloth | Gas | tensor([1.4924e-02, 3.0396e-02, 8.8559e-01, 8.5396e-02, 5.0591e-05, 5.4732e-04, 8.9289e-04]) | 0.8238886594772339 | . 4 No_mask! | - | tensor([0.7779, 0.0738, 0.0020, 0.0144, 0.0422, 0.0209, 0.0432]) | 0.7935909628868103 | . 5 KN95;Shield | Shield | tensor([0.3309, 0.0627, 0.0055, 0.0164, 0.1653, 0.0027, 0.6454]) | 0.7432813048362732 | . 6 Cloth;KN95;Medical | Cloth | tensor([1.7814e-04, 9.5576e-01, 3.8822e-05, 1.8135e-02, 3.3463e-01, 2.3627e-03, 9.8615e-04]) | 0.7362056970596313 | . 7 Gas | - | tensor([0.7025, 0.0204, 0.0410, 0.0281, 0.0105, 0.0157, 0.0047]) | 0.6411075592041016 | . 8 KN95;Shield | Cloth;Shield | tensor([0.0148, 0.5299, 0.0042, 0.0596, 0.0646, 0.0011, 0.7471]) | 0.5646962523460388 | . 9 Cloth;KN95 | Cloth;Medical | tensor([0.0164, 0.7377, 0.0166, 0.0930, 0.6964, 0.0030, 0.0013]) | 0.5584717988967896 | . 10 Medical;Shield | Medical | tensor([7.7260e-05, 2.4786e-04, 2.5288e-06, 1.0900e-05, 9.9973e-01, 1.1516e-06, 2.5888e-02]) | 0.5220851898193359 | . 11 Medical;Shield | Medical | tensor([8.0212e-05, 1.3874e-04, 4.7152e-06, 6.7401e-06, 9.9989e-01, 1.2644e-06, 3.4239e-02]) | 0.48210400342941284 | . 12 Cloth | | tensor([1.8111e-04, 6.7206e-02, 2.0173e-03, 1.7350e-02, 4.4227e-01, 1.1182e-03, 5.5856e-02]) | 0.4803086221218109 | . 13 Medical | Cloth;Medical | tensor([6.0449e-03, 9.4671e-01, 9.7263e-05, 4.8402e-02, 8.8149e-01, 6.1641e-04, 5.9388e-03]) | 0.4457746148109436 | . 14 Cloth;Shield | Shield | tensor([0.0735, 0.1129, 0.0115, 0.0598, 0.1168, 0.1199, 0.6379]) | 0.4332104027271271 | . 15 - | | tensor([1.4689e-01, 4.0333e-01, 3.8488e-04, 2.5451e-02, 1.7683e-02, 3.5612e-01, 2.9692e-03]) | 0.4173838198184967 | . 16 Cloth;Shield | Cloth | tensor([5.3234e-04, 8.4760e-01, 3.0976e-03, 5.6098e-02, 3.5594e-01, 1.9782e-03, 1.0929e-01]) | 0.4117661714553833 | . 17 Medical | Shield | tensor([3.6909e-04, 5.1937e-03, 6.3702e-03, 3.9115e-02, 4.6684e-01, 1.4459e-03, 8.4203e-01]) | 0.38006311655044556 | . 18 Medical | Medical;Shield | tensor([1.4350e-03, 5.0586e-03, 9.7197e-04, 2.2856e-03, 9.0900e-01, 3.1733e-04, 9.1633e-01]) | 0.3694818615913391 | . 19 - | Cloth | tensor([0.3624, 0.7249, 0.0532, 0.0152, 0.0029, 0.0012, 0.1324]) | 0.36024340987205505 | . 20 Medical | | tensor([1.0746e-04, 1.6511e-02, 8.3923e-04, 3.2906e-01, 1.5616e-01, 2.8831e-04, 1.9375e-02]) | 0.32763299345970154 | . 21 Shield | | tensor([0.0384, 0.1765, 0.0033, 0.0331, 0.1166, 0.1871, 0.2049]) | 0.3123726546764374 | . 22 Gas | Cloth | tensor([2.6330e-03, 5.6460e-01, 2.9816e-01, 2.8259e-02, 5.7453e-03, 5.7981e-03, 1.5344e-05]) | 0.29778796434402466 | . 23 Shield | No_mask!;Shield | tensor([1.8141e-03, 8.6844e-03, 2.3820e-04, 3.2092e-02, 8.4428e-03, 7.2501e-01, 6.3948e-01]) | 0.25571224093437195 | . 24 Medical;Shield | Medical | tensor([6.6729e-05, 2.0053e-05, 2.2359e-06, 5.1963e-06, 9.9997e-01, 5.4061e-07, 1.7936e-01]) | 0.24549484252929688 | . 25 Gas | | tensor([0.0445, 0.4098, 0.4082, 0.0930, 0.0095, 0.0127, 0.0837]) | 0.239467591047287 | . 26 Medical;Shield | Medical | tensor([4.8403e-05, 1.8774e-04, 1.6524e-05, 2.7448e-05, 9.9934e-01, 5.4019e-06, 1.8736e-01]) | 0.2393771857023239 | . 27 Medical;Shield | Medical | tensor([1.2438e-04, 1.0014e-05, 4.7314e-06, 3.9429e-06, 9.9997e-01, 1.1959e-06, 1.9395e-01]) | 0.2343320995569229 | . 28 Cloth | Cloth;Medical | tensor([1.5245e-02, 9.7492e-01, 7.8498e-05, 8.6758e-02, 7.7415e-01, 1.3900e-04, 2.3870e-04]) | 0.23140935599803925 | . 29 No_mask! | | tensor([0.3722, 0.0604, 0.0008, 0.0114, 0.0044, 0.3931, 0.0162]) | 0.21351192891597748 | . 30 Medical;Shield | Medical | tensor([7.7823e-05, 3.0472e-04, 7.8055e-05, 2.0316e-04, 9.9646e-01, 1.2870e-05, 2.2650e-01]) | 0.2127458155155182 | . 31 Medical;Shield | Medical | tensor([1.5801e-04, 2.7898e-04, 3.7677e-05, 3.9934e-05, 9.9944e-01, 8.0454e-06, 2.5164e-01]) | 0.1972629874944687 | . 32 Shield | Cloth;Shield | tensor([8.5926e-03, 6.1524e-01, 5.3632e-03, 2.3268e-02, 8.1071e-03, 5.4811e-04, 6.8703e-01]) | 0.19667738676071167 | . 33 KN95 | | tensor([0.0018, 0.0974, 0.0242, 0.3349, 0.0514, 0.0027, 0.0128]) | 0.1844436228275299 | . 34 Cloth;Medical | Cloth | tensor([1.8994e-04, 9.5007e-01, 2.0903e-04, 3.6779e-02, 3.4507e-01, 2.3358e-02, 6.6488e-04]) | 0.16820193827152252 | . 35 Shield | Shield | tensor([0.1247, 0.2386, 0.0015, 0.0478, 0.0223, 0.0024, 0.5072]) | 0.16572348773479462 | . 36 Medical;Shield | Shield | tensor([5.2771e-04, 3.7455e-02, 2.6122e-02, 1.2338e-02, 3.7254e-01, 1.2216e-03, 9.6325e-01]) | 0.15766814351081848 | . 37 Cloth | | tensor([3.1620e-04, 4.4886e-01, 3.6443e-03, 3.3457e-02, 3.9798e-03, 1.5899e-01, 2.9744e-03]) | 0.14559409022331238 | . 38 Gas | Gas | tensor([0.0038, 0.0341, 0.6053, 0.0717, 0.0194, 0.0156, 0.2760]) | 0.13902735710144043 | . 39 No_mask! | No_mask! | tensor([0.0397, 0.3198, 0.0012, 0.0885, 0.0116, 0.7440, 0.1094]) | 0.13470366597175598 | . Second Try - Review . Manual Testing . The model was manually tested against a series of images, which were itentionally created to confuse it, and to trick it into providing false predictions. Suprisingly, the model ended up tricking me, as it fared perfectly, identifying each and every case correctly, or, at least, not wrongly. . learn0.predict(&#39;WhatsApp Image 2020-09-13 at 8.34.08 PM.jpeg&#39;) . ((#1) [&#39;KN95&#39;], tensor([False, False, False, True, False, False, False]), tensor([3.7567e-04, 1.4866e-02, 1.9021e-03, 9.3934e-01, 1.2692e-03, 6.1743e-02, 1.6483e-03])) . learn0.predict(&#39;WhatsApp Image 2020-09-13 at 8.34.23 PM.jpeg&#39;) . ((#1) [&#39;KN95&#39;], tensor([False, False, False, True, False, False, False]), tensor([1.9798e-04, 2.5388e-03, 7.8868e-04, 9.8569e-01, 4.1836e-03, 1.0470e-03, 1.8771e-03])) . # In this case, I passed on to it an image of a few family members of mine, and it did not provide an answer. # That is correct, since there is no fitting category learn0.predict(&#39;family.jpg&#39;) . ((#0) [], tensor([False, False, False, False, False, False, False]), tensor([2.4998e-04, 4.1010e-03, 6.8737e-03, 3.9971e-01, 6.2634e-04, 3.4899e-01, 1.5314e-02])) . learn0.predict(&#39;xr.png&#39;) . ((#1) [&#39;Medical&#39;], tensor([False, False, False, False, True, False, False]), tensor([7.4280e-04, 1.4337e-03, 3.9989e-04, 7.5603e-04, 9.9519e-01, 3.1935e-05, 6.4606e-02])) . # Again, a friend of mine was not identified as a mask category. I would also consider correct a &quot;No_mask!&quot; prediction. learn0.predict(&#39;boom.PNG&#39;) . ((#0) [], tensor([False, False, False, False, False, False, False]), tensor([0.3424, 0.0293, 0.0019, 0.0444, 0.0084, 0.0394, 0.0300])) . # Lastly, an old-school photo of a man did not confuse the model either. learn0.predict(&#39;t.PNG&#39;) . ((#1) [&#39;No_mask!&#39;], tensor([False, False, False, False, False, True, False]), tensor([0.0467, 0.0834, 0.0065, 0.1329, 0.0323, 0.5063, 0.0385])) . In total . The model did exceptionally in the Manual Testing phase. It did not make any wrong predictions. However, these results and the top loses propelled me to understand what is really happening; when you increase the resolution, the model gets more and more inaccurate at recognizing the types of what it is searching for, and their characteristic differences. . That means that as the model gets better at figuring out how to not classify an image that does not contain a mask as a type of a mask it gets worse at locating the differencies between types of masks. The consequencies are that the model is more accurate and confident about things not being, rather than them being, what it is searching for. . # As a clarification, these are the possible categories for the time being that the model has been trained to recognize learn0.dls.vocab . (#7) [&#39;-&#39;,&#39;Cloth&#39;,&#39;Gas&#39;,&#39;KN95&#39;,&#39;Medical&#39;,&#39;No_mask!&#39;,&#39;Shield&#39;] . Anyways, it&#39;s a fairly good model, with potential practical use. That&#39;s why I decided to export it. . learn0.export(&#39;masksv3_modelv01.pkl&#39;) . Conclusion . In this post, I covered as well as I could the process which I followed to create the &quot;masksv3_modelv01.pkl&quot; model. . The journey, however, is far from over. To achieve the model displayed at https://protected-mountain-28715.herokuapp.com/ as a free, available for anyone to use application, I had to recreate the dataset a few more times, as well as to explore distinct approaches and peculiar ideas (out of which the most important could be &quot;Intended Noise&quot;), among other things. . Make sure to get in touch with me for any suggestions, comments or corrections. A respectable and thoughtful comment is always welcome from anyone. .",
            "url": "https://ntoump.github.io/blog/deep%20learning/jupyter/cnn/intended%20noise/2020/09/17/masksv3_PART_1.html",
            "relUrl": "/deep%20learning/jupyter/cnn/intended%20noise/2020/09/17/masksv3_PART_1.html",
            "date": " • Sep 17, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Nikolaos Toumpanakis . I am an undergraduate student with a wide area of interests, especially in technology, philosophy/mathematics, international relations, and economics and finance. . Blog Focus . This blog will mostly focus on the following vast fields: Artificial Intelligence, Machine/Deep Learning, and Data Science. . Goal . I created this blog (using fastpages) to share my technological findings and interests with the world. . My plans . I plan on delving deeply into AI and Deep Learning in the next few months, as I am fascinated by its seemingly infinite possible applications, as well as its future prospects. . Blog Content . Expect to read about my experiments, my ideas, and my projects, as I progress through the fastai course. . Subscription . You can subscribe to receive an email notification every time a post is made here. . About me . As I am still but an ambitious student, you will encounter mistakes and potentially failures, both in my sometimes unorthodox methodologies, and even in my code. Slight inefficiencies will be ubiquitous, especially for the first few months. My personal aspiration, though, is that through thorough study and by maintaining a relentless and tenacious attitude, I will be able to improve myself and my capabilities beyond what I now consider possible. . Contact . Business Email: toump.nick@gmail.com | GitHub | YouTube . Feel free to reach out for a question or a comment; I am always looking for interesting people to engage in discourse with. .",
          "url": "https://ntoump.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ntoump.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}